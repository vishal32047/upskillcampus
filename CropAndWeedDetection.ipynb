{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8zRgH-64XIQ8"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def detect_with_bboxes(image_path, bounding_boxes, class_names):\n","    img = cv2.imread(image_path)\n","    height, width, channels = img.shape\n","\n","    colors = np.random.uniform(0, 255, size=(len(class_names), 3))\n","    font = cv2.FONT_HERSHEY_SIMPLEX\n","\n","    for i, bbox in enumerate(bounding_boxes):\n","        x, y, w, h = bbox\n","        label = class_names[i]\n","\n","        color = colors[i]\n","        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n","        cv2.putText(img, label, (x, y - 5), font, 0.5, color, 2)\n","\n","    det = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    plt.figure(figsize=(12, 8))\n","    plt.axis('off')\n","    plt.imshow(det)\n","\n","    # Save the detected image\n","    output_path = 'output.jpg'\n","    det_bgr = cv2.cvtColor(det, cv2.COLOR_RGB2BGR)\n","    cv2.imwrite(output_path, det_bgr)\n","\n","    return plt\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-OH2IrYpXShi"},"outputs":[],"source":["import os\n","\n","# Path to the folder containing images and bounding box data\n","data_folder = '/content/drive/MyDrive/Crop and weed detection/agri_data/data'\n","\n","# Path to the file containing class names\n","class_names_file = '/content/drive/MyDrive/Crop and weed detection/classes.txt'\n","\n","# List all image files in the data folder\n","image_files = [f for f in os.listdir(data_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n","\n","# Load class names from the file\n","with open(class_names_file, 'r') as f:\n","    classes = [line.strip() for line in f.readlines()]\n","\n","# Limit the loop to 20 iterations\n","num_iterations = 20\n","\n","# Loop through each image for a maximum of 20 iterations\n","for i, image_file in enumerate(image_files):\n","    if i >= num_iterations:\n","        break\n","\n","    image_path = os.path.join(data_folder, image_file)\n","\n","    # Assuming you have a corresponding text file for each image with bounding box coordinates\n","    bbox_file = os.path.splitext(image_file)[0] + '.txt'\n","    bbox_path = os.path.join(data_folder, bbox_file)\n","\n","    if os.path.isfile(bbox_path):\n","        bounding_boxes = []\n","        class_indices = []\n","\n","        with open(bbox_path, 'r') as f:\n","            lines = f.readlines()\n","            for line in lines:\n","                values = line.strip().split()\n","                class_index = int(values[0])  # Assuming the class index is the first value in each line\n","                x, y, w, h = map(float, values[1:])  # Assuming the remaining values are bounding box coordinates\n","                bounding_boxes.append([int(x), int(y), int(w), int(h)])\n","                class_indices.append(class_index)\n","\n","        # Map class indices to class names\n","        class_names = [classes[idx] for idx in class_indices]\n","\n","        # Call the detection function\n","        plt = detect_with_bboxes(image_path, bounding_boxes, class_names)\n","        plt.show()\n"]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","def draw_bboxes(image_path, bounding_boxes, class_names):\n","    img = cv2.imread(image_path)\n","\n","    for i, bbox in enumerate(bounding_boxes):\n","        x, y, w, h = bbox\n","        label = class_names[i]\n","\n","        color = (0, 255, 0)  # Green color for bounding boxes\n","        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n","        cv2.putText(img, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n","\n","    plt.figure(figsize=(12, 8))\n","    plt.axis('off')\n","    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","    plt.show()\n","\n","# Path to the folder containing images and bounding box data\n","data_folder = ''\n","\n","# List all image files in the data folder\n","image_files = [f for f in os.listdir(data_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n","\n","# Loop through each image\n","for image_file in image_files:\n","    image_path = os.path.join(data_folder, image_file)\n","\n","    # Assuming you have a corresponding text file for each image with bounding box coordinates\n","    bbox_file = os.path.splitext(image_file)[0] + '.txt'\n","    bbox_path = os.path.join(data_folder, bbox_file)\n","\n","    if os.path.isfile(bbox_path):\n","        bounding_boxes = []\n","        class_names = []\n","\n","        with open(bbox_path, 'r') as f:\n","            lines = f.readlines()\n","            for line in lines:\n","                values = line.strip().split()\n","                class_index = int(values[0])  # Assuming the class index is the first value in each line\n","                x, y, w, h = map(float, values[1:])  # Assuming the remaining values are bounding box coordinates\n","                bounding_boxes.append([int(x), int(y), int(w), int(h)])\n","                class_names.append(classes[class_index])\n","\n","        # Call the function to draw bounding boxes\n","        draw_bboxes(image_path, bounding_boxes, class_names)\n"],"metadata":{"id":"EshSIX8-d3_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torch torchvision pillow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XJ_Kv6PPhIrN","executionInfo":{"status":"ok","timestamp":1692527804286,"user_tz":-330,"elapsed":6244,"user":{"displayName":"Vishal Kumar","userId":"09073838214466913131"}},"outputId":"93e49dd3-2fe3-4ffd-cae4-f1b4102330e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","from torchvision.models.detection import fasterrcnn_resnet50_fpn\n","from torchvision.transforms import functional as F\n","from PIL import Image\n","from torchvision.models.detection import fasterrcnn_resnet50_fpn\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","\n","# Define classes\n","CLASSES = ['crop', 'weed']\n","\n","# Load pre-trained Faster R-CNN model\n","\n","model = fasterrcnn_resnet50_fpn(pretrained=True)\n","\n","model.eval()\n","\n","# Load the directory paths for images and bounding box data\n","image_dir = '/content/drive/MyDrive/Crop and weed detection/agri_data/data'\n","bbox_dir = '/content/drive/MyDrive/Crop and weed detection/classes.txt'\n","\n","# Process each image\n","for img_filename in os.listdir(image_dir):\n","    if img_filename.endswith(\".jpg\"):  # Assuming images are in jpg format\n","        img_path = os.path.join(image_dir, img_filename)\n","        bbox_path = os.path.join(bbox_dir, img_filename.replace('.jpg', '.txt'))\n","\n","        # ... (rest of the code)\n","\n","        # Load image\n","        img = Image.open(img_path).convert(\"RGB\")\n","\n","        # Create figure and axes\n","        fig, ax = plt.subplots(1)\n","        ax.imshow(img)\n","\n","        # Draw bounding boxes\n","        for bbox in boxes:\n","            x_min, y_min, x_max, y_max = bbox\n","            rect = patches.Rectangle(\n","                (x_min, y_min), x_max - x_min, y_max - y_min,\n","                linewidth=1, edgecolor='r', facecolor='none'\n","            )\n","            ax.add_patch(rect)\n","\n","        # Show image with bounding boxes\n","        plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1VszcsSShQzo","executionInfo":{"status":"ok","timestamp":1692528391018,"user_tz":-330,"elapsed":2600,"user":{"displayName":"Vishal Kumar","userId":"09073838214466913131"}},"outputId":"4a1dbb69-5a09-4c9f-9694-2a878d69227b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","# Define the dataset\n","dataset = datasets.ImageFolder(root=\"/content/drive/MyDrive/Crop and weed detection/agri_data/data\", transform=transforms.ToTensor())\n","\n","# Split the dataset into training and validation sets\n","train_dataset, val_dataset = torch.utils.data.random_split(dataset, [80, 20])\n","\n","# Define the RCNN model\n","model = FasterRCNN(backbone=\"resnet50\")\n","\n","# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Train the model\n","for epoch in range(10):\n","    for i, (images, labels) in enumerate(train_dataset):\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Evaluate the model on the validation set\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        for images, labels in val_dataset:\n","            outputs = model(images)\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        accuracy = 100 * correct / total\n","        print(f\"Epoch {epoch + 1}: accuracy = {accuracy}\")\n","\n","# Make predictions\n","image = torch.load(\"/content/drive/MyDrive/Crop and weed detection/agri_data/data/agri_0_1009.jpeg\")\n","outputs = model(image)\n","_, predicted = outputs.max(1)\n","\n","if predicted == 0:\n","    print(\"The image is a crop.\")\n","else:\n","    print(\"The image is a weed.\")\n"],"metadata":{"id":"IJucX4Dfjl05"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"16MpriO7mSTYqzKJuVdjyufClwkS-DiG-","authorship_tag":"ABX9TyOOkS7IGgwT6KDv87uZ/2sp"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}